----------------------------
| 2: Processes and threads | 
----------------------------
------------------------
---- 2.1: Processes ----
------------------------
- upon system boot, many processes are started at the same time
- in multiprogramming system cpu switches from process to process very quickly
- process model
  - all runnable software on computer organized into sequential processes
  - process: instance of an executing program
  - each process has its own virtual cpu
  - multiprogramming: switching back and forth quickly between processes
  - process is activity of some kind. it has program, input, output, and state. program is something that may be stored on disk not doing anything
  - 4 key events cause processes to be created
    - System initialization
    - Execution of process creation sys call by running process
    - User request to create new process
    - Initialization of a batch job
- daemons: processes that stay in the background to handle some activity
- fork will create an exact clone of the calling process in unix
  - have same memory image, environment strings, and same open files
  - child run execve to change its memory image and run a new program
  - parent and child PROCESS have their own distinct address space
- process terminates from one of the following conditions
  - Normal exit (voluntary): finish their work 
  - Error exit (voluntary): call exit
  - Fatal error (involuntary): discover fatal error or bug
  - Killed by another process (involuntary): kill command
- processes form hierarchy, kind of like hydra
  - init process after boot in first
- three process states
  - Running (actually using the cpu at that instant)
  - Ready (runnable, temporarily stopped to let another process run)
  - Blocked (unable to run until some other external event happens)
- state changes possible
  - running -> blocked : process blocks for input
  - running -> ready: scheduler picks another process to run
  - ready -> running: scheduler picks this process
  - blocked -> ready: input becomes available
- os maintains proces table with one entry per process
  - contains important imformation about the processes state
- each I/O class has location called interrupt vector that contains address of ISR (interrupt service routine)
  - on interrupt, computer jumps to address in interrupt vector then run ISR
- cpu utilization/degree of multiprogramming = 1 - p ** n


----------------------
---- 2.2: Threads ----
----------------------
- threads are basically mini processes
  - parallel entities share address space and data among themselves
  - lighter weight than processes, faster to create and destory
- concepts of a process
  - resource grouping
    - group together related resources such as addr space, open files, alarms, etc
  - execution
    - process is a thread of execution. has pc, registers, vars, stack, etc
- processes are used to group resources together, threads are the entities scheduled for execution on the cpu
- threads add to the process model by allowing multiple executions to take place in the same process environment
- multithreading: allowing multiple threads in the same process
  - cpu switches rapidly back and forth among threads
- threads share the same address space, but each have their own stack
- thread can be running, blocked, ready, terminated like a process
- threads can be implemented in user space or kernel space
- user space
  - kernel knows nothing about them and just mamanges single threaded process
  - run on top of run time system
  - each process needs own thread table to keep track of threads in that process, managed by run time system
  - thread switch very very fast
  - process can have customized scheduling algorithm
  - problems when implementing blocking calls and have to manually yield
  - still have to trap to kernel to handle system call
- in kerenel
  - dont need a run time system
  - kernel has single thread table that keeps track of all threads
    - does creation and deletion here
    - holds each threads registers, state, etc
  - cost of sys call to make thread is very high
- hybrid approach
  - use kernel level threads and multiplex user level threads onto some or all of them


-----------------------------------
---- 2.3: Event Driven Servers ----
-----------------------------------
- Threads: parallelism: blocking system calls
- Single threaded process: No parallelism, blocking system calls
- Finite-state machine/event-driven: parallelism, nonblocking system calls, interrupts


-------------------------------------------------------------
---- 2.4: Synchronization and Interprocess Communication ----
-------------------------------------------------------------
- Three problems with IPC
1) How to pass info 
2) How to make sure two or more process/threads to not get in each others way
3) Sequencing when dependencies are present
- Race conditions: when two or more processes are reading/writing to some shared data and the final result depends on who runs when
  - need to prohibit more than one process from reading/writing shared data at same time, mutual exclusion
- Critical section: part of program where shared memory is accessed
- proposals for mutual exclusion
1) Disabiling Interrupts
- on single processor system, just disable interrupts after entering critical section and re-enable after leaving
- means cpu will not switch processes
- unwise to give user process power to turn off interrupts
- if multiprocessor, only affts CPU that disabled it
- disabiling interrupts only useful in OS itself
2) Lock variables
- lock variable set to zero and when process wants to enter CS, test then set
- non atomic so still suffers race conditions
3) Strict Alternation
- keep track of whos turn it is via some variable
- busy waiting, continuously testing variable
- not good when one process is much slower
4)Peterson's Solution
- before using shred var, call enter_region with process num
- call leave_region to leave and signal that it is safe to enter
5) TSL Instruction
- test and set lock, read contents of memory word lock and store nonzero value at memory address
- atomic instruction
- alternative instr is XCHG, which x86 use
- requires busy waiting
- sleep: system call that causes caller to block/be suspended until another process wakes it up
- wakeup: takes process to be awakened and wakes it up
- producer-consumer/bounded buffer problem
  - producer puts stuff in buffer, consumer takes it out
  - producer go to sleep if buffer is full, consumer go to sleep if buffer is empty
  - race condition on the count variable that tracks amount of items in buffer, can screw up wakeups
- semaphores can be used to solve the producer consumer problem, prevents the lost wakeup
- mutex semaphore is used for mutaul exclusion
- full and empty semaphores are for synchronization
- mutex is a semaphore that does not count
  - shared variable tath can be in one of two states: locked or unlocked
  - if thread fails to get a lock, it yields
- futex: fast user space mutex
  - implements locking but avoids going into kernel unless necessary
  - consists of kernel service and user library
  - kernel service has wait queue that allow smultiple processes to wait on a lock, kernel must explicitly unblock them
- monitor: collections of prodecures, vars, and data structurs that are gropus together in special module/package
  - process may call prodecures in monitor byt cannot directly access monitors intenral data structures
  - a monitor is a language concept
  - only one processes can be in a monitor at anytime
- condition variables: signal and wait
  - if signal done on cv where several processes are waiting, only one woken up
- message passing: ipc using send and receive


-------------------------
---- 2.5: Scheduling ----
-------------------------
- if computer multiprogrammed, may processes/threads computing for cpu 
- scheduler chooses which process to run next via a scheduling algorithm

---- Introduction to Scheduling ----
------------------------------------
- for most users, scheduling is simple due to a lot of waiting around and only using on process at a time
- context switch: switch from user more to kernel mode, save state of current process, restore state of other process
  - context switches are expensive, cycle wise, to preform
- processes alternative bursts of computing with bursts of I/O
- compute bound: long cpu bursts and infrequent I/O waits
- i/o bound: short cpu bursts and frequency i/o waits
- when should a scheduling decision be made?
1) when a new process is created
2) when a process exits
3) when process blocks
4) when I/o interrupt occurs
- nonpreemptive scheduling algorithm: picks process to run and lets it run until it blocks
- preemptive scheduling algorithm: picks a process and lets it run for some max time,
if still running at the end of the interval it will suspend it and pick a new process
- scheduling algorithms should be fair, enforce the policy, and keep all parts of the system busy

---- Schedling in Batch Systems ----
------------------------------------
1) first come first serve
- nonpreemptive
- processes assigned the cpu in the order they request it
- remove from front and add to back
- easy to understand and simple to implement but not good for heavy I/O
2) shortet job first
- nonpreemptive
- when runtimes are known, can just pick shortest one
- optimal only when all jobs are available simultaneously
3) Shortest remaining time next
- preemptive
- choose process whose remaining time is the shortest
- run time has to be known in advanced
- allows short jobs to get good service

---- Scheduling in Interactive Systems ----
-------------------------------------------
1) Round Robin Scheduling
- each process assigned time interval called quantum
  - process allowed to run during its quantum
- if still running at end of quantum, preempted
- if block while quantum not up, preempted
- choosing correct quantum very important
  - too short -> too many context swtich 
  - too high -> poor response to short req
  - 20-50 msec is often reasonable
2) Priority Scheduling
- each process is assigned a priority
- process with highest priority gets to run
- scheudler may decrease priority to prevent it to run idenfinitely
- could also use a time quantum
- can group processes into priority classes
  - use priority scheduling among classes
  - use round robin within class
3) Multiple queues
- setup priority classes with different quantum levels
- move down a class each time quantum expires
4) Guaranteed Schedling
- with n processes, each get 1/n cpu
- system tracks how my cpu each process has had
5) Lottery Scheduling
- give processes lottery tickets for various system resources
- upon scheduling decision, lottery ticket chosen at random and process w/ ticket gets the resource
- can give processes multiple tickets based on priority
- "all processes are equal, but some processes are more equal"
6) Fair-Share Scheduling
- some systems take into account which user owns a process before scheduling it
- each user allocated some fraction of cpu

---- Scheduling in Real Time Systems ----
-----------------------------------------
- external system generates stimuli in which device must resonse
- hard real time: absolute deadlines that must be met
- soft real time: missing occasional deadline is tolerable
- events can be periodic or aperiodic
- static scheduling makes scheduling decisions before system starts running
  - must have perfect information
- dynamic schedling makes scheduling decision at run time

---- Policy vs Mechanism ----
-----------------------------
- scheduler parameterized in some way, but params can be filled in by user process 
  - ex: syscall to change process priority

---- Thread Scheduling ----
---------------------------
- may have many processes each running many threads
  - two levels of parallelism: processes and threads
- scheduling different in user level threads or kernel level threads
1) user level threads
- kernel not aware of the threads and schedules processes like normal
- runtime makes thread scheduling decisions
- absence of clock to interrupt thread
- fast context switch
- I/O suspends the entire process
2) kernel level threads
- kernel picks a thread to run
- does not care which thread the process belongs to
- slow context switch
- I/O does not suspend the entire process

-------------------------
---- Chapter Writeup ----
-------------------------
Processes and threads are the workhorses of an operating system. All runnable software on a computer
are organized into sequential processes. A process can be thought of an as an instance of an executing program,
but it is not just the running program. It also consists of a set of resources that contain things such 
as open files, execution state, pending signals, address space, etc. A thread can be thought of as a lightweight process.
While a process does not share address space with other processes, threads do. Although, they do maintain some
of their own state such as the program counter, stack, etc. A new process is created via the fork() system call.
In summary, processes are used to group resources together and threads are scheduled for execution. 

Scheduling is a core functionality of an operating system. Given a set of threads, it must schedule each one onto
physical hardware to be executed. When there are less threads than available hardware threads, scheduling is easy.
When there are more threads then hardware threads, the scheduler must use some algorithm to ensure fair and efficient
use of computer resources that is distributed among all of the threads. There are non preemptive scheulders in which
the thread itself must yield and there are preemptive in which the os will suspend a running process and run another one.
There are varoius scheduling algorithms that each have their own pros and cons. 


----------------------
---- Memory notes ----
----------------------
- Process: instance of a running program, including current values of program counter, registers, and variables
- Process model: abstraction that provides illusion of many sequential processes running in parallel on a single CPU
- Multiprogramming: technique of having multiple processes take turns using the CPU
- Process creation events: system initialization, execution of process creation system call, user request, initiation of batch job
- Process termination conditions: normal exit, error exit, fatal error, killed by another process
- Process states: running (using CPU), ready (runnable, temporarily stopped), blocked (unable to run until some external event happens)
- Process table: kernel data structure containing information about all processes
- Process table entry contents: process state, program counter, stack pointer, memory allocation, open file status, accounting info, scheduling info
- Thread: lightweight process within a process, shares address space but has own program counter and stack
- Threads vs processes: threads share address space and resources, processes have separate address spaces
- User-level threads: managed by user-space runtime system, kernel unaware
- Kernel-level threads: managed by kernel, can be scheduled independently
- Thread advantages: sharing of resources, economy, utilization of multiprocessor architectures
- Event-driven programming: alternative to threads, uses finite state machine and non-blocking I/O
- Interprocess communication (IPC): mechanisms for processes to synchronize and communicate
- Race condition: situation where multiple processes access and manipulate shared data concurrently, with the outcome depending on the order of execution
- Critical region: part of the program where shared memory is accessed
- Mutual exclusion: ensuring that if one process is using a shared resource, the other processes will be excluded from doing the same
- Busy waiting: continuously testing a variable until some value appears
- Semaphores: synchronization primitive with down and up operations
- Monitors: high-level synchronization primitive with procedures, variables, and data structures grouped together
- Message passing: IPC method using send and receive primitives
- Barriers: synchronization mechanism for groups of processes
- Scheduling: process of deciding which process runs next
- Scheduling algorithms:
  - First-Come, First-Served (FCFS): non-preemptive, runs processes in order of arrival
  - Shortest Job First (SJF): non-preemptive, runs shortest job next
  - Shortest Remaining Time Next: preemptive version of SJF
  - Round Robin (RR): each process gets a small unit of CPU time, cycles through all processes
  - Priority Scheduling: assigns priority to each process, runs highest priority
  - Multiple Queues: multiple priority queues, processes move between queues
  - Guaranteed Scheduling: makes real promises about performance
  - Lottery Scheduling: processes get lottery tickets, winners get CPU time
  - Fair-Share Scheduling: users allocated portion of CPU time
- Real-time scheduling: ensures that processes meet deadlines
- Hard real-time systems: absolute deadlines that must be met
- Soft real-time systems: missing occasional deadline is tolerable
- Thread scheduling: differs for user-level and kernel-level threads
- User-level thread scheduling: done by runtime system, no kernel involvement
- Kernel-level thread scheduling: done by kernel, can take into account which process the thread belongs to
- Policy vs. Mechanism: separation of scheduling algorithm (mechanism) from the parameters that control it (policy)
- Context switch: saving the state of one process and loading the state of another
- Process creation: fork() in UNIX, CreateProcess() in Windows
- Process hierarchy: parent-child relationship in UNIX, flat structure in Windows
- Signals: software interrupts for interprocess communication
- Interprocess communication challenges: lost wake-ups, race conditions
- Dekker's algorithm: first correct software solution for mutual exclusion
- Peterson's solution: simplified mutual exclusion algorithm
- TSL (Test and Set Lock) instruction: atomic instruction for implementing mutual exclusion
- Producer-consumer problem: classic synchronization scenario
- Readers-writers problem: synchronization scenario with multiple readers and writers
- Priority inversion: scheduling anomaly where high-priority process is indirectly preempted by medium-priority process
- POSIX threads (Pthreads): standardized threading API
- Thread pooling: creating a pool of worker threads to handle tasks
- Scheduler activations: mechanism for kernel to notify user-level thread scheduler about threading events
- Memory barriers: instructions to ensure memory access ordering
- Read-Copy-Update (RCU): synchronization mechanism allowing lock-free reads
- Completely Fair Scheduler (CFS): Linux scheduling algorithm based on fair allocation of CPU time