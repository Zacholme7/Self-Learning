------------------------
| 3: Memory Management | 
------------------------
- "programs expand to fill memory available to them"

------------------------------------
---- 3.1: No Memory Abstraction ----
------------------------------------
- 1980s and before every program just say entire physical memory

---- Running Multiple Programs without a memory abstraction ----
---------------------------------------------------------------- 
- have to save memory to file and bring in other program


--------------------------------------------------
---- 3.2: A Memory Abstraction: Address space ----
--------------------------------------------------

---- The Notion of an address space ----
----------------------------------------
- to allow multiple application in memory, protection and relocation have to be solved
- address space: set of addresses that a process can use to address memory
  - created kind of abstract memory for programs to use
- each process has its own address space
- simple solution to address space
  - dynamic relocation
  - map each processes address space onto different part of physical memory
  - cpu has base and limit reg
  - base reg has addr of start of program, limit loaded with length of program
  - add addr loc to base reg and make sure within limit
  - disadvantage is additional comparison for each memory reference

---- Swapping ----
------------------
- total amount of RAM needed often much more than what can fit in mem
- swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- creates holes in memory but can use memory compaction to fill in
- allocate initial program and room to grow

---- Managing Free Memory ----
------------------------------
- can use bitmaps and free lists to keep track of memory usage
1) bitmap: memory divded into allocation units in size from a few words to several kilobytes
  - each allocation unit has bit, 0 for free and 1 for occupied
  - searching bit map is slow
2) linked list: linked list of allocated and free memory segements
  - segement either process or empty hole between two processes
  - several algorithms for allocation. first fit, next fit, best fit, quick fit, ...


-----------------------------
---- 3.3: Virtual Memory ----
-----------------------------
- there is a need to run programs that are too large to fit in memory
- virtual memory: program has its own addr space that is broken into pages
  - page: a contiguous range of addresses mapped onto physical memory
  - not all pages have to be present

---- Paging ----
----------------
- virtual addresses: program generated addresses that form the virtual address space
  - go to the mmu
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- present/absent bit keeps track of which pages are physically present in memory
- if MMU sees page is unmapped, page fault occurs
  - writes unused page to disk and brings in needed page
- page number used as index into page table, other bits used as offset
- process
1) program issue adress to access byte in page
2) mmu checks page table to see if page is mapped to physical frame
  - if mapped, retrieve data
3) if not mapped, issue page fault bc it needs to load the page into memory
4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped
6) restart the instruction

---- Page Tables ----
---------------------
- virtual address split into virtual page number (high order bits) and offset (low order bits)
- virtual page number used as index into page table to find entry for that virtual page
  - if found, page frame number attatched to offset to form physical address for memory
- purpose of page table it so map virtual pages onto page frames
- structure of a page table entry
  - varies but general form is consistent
  - page frame number
  - present/absent bit: indicates wherther entry is valid and can be used
  - protection: tells what kind of actions are permitted
  - supervisor: indicates whether page is accessible to only privilged code
  - modified: tracks if page was modified, must be written back if so
  - referenced: keeps track of references, needed to decided which to exict
  - cache disable: can turn off caching
- page table only holds info the hardware needs to tranlsate virtual addr into physical

---- Speeding up Paging ----
------------------------------
- 2 issues must be addresses
  1) mapping from virtual -> physical must be fast
  2) even if virtual addr space very large, page table must not be large
- mapping has to be done on every memory reference
- most programs tend to make a large number of references to a small number of pages
  - only fraction of pte are heavily read, rest barely used
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table
  - usually inside the mmu
- when address presented to mmu for tranlsation
1) hardward checks to see if virtual page number in TBL (done in parallel via hardware)
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory
3) if no match found, does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 

---- Page Tables for Large Memories ----
----------------------------------------
1) Multilevel page tables
- multiple levels of page tables
- addr broken into indexes, pt1, pt2, ...
- can keep nesting levels
2) inverted page tables
- one entry per page frame in real memory rather than one entry per page of virtual addr space
- entry keeps track of which (process, virtual page) is located in page frame
- downside is that it must search entire table
  - TLB and hashing can speed up


------------------------------------------
---- 3.4: Page Replacement Algorithms ----
------------------------------------------

---- The Optimal Page Replacement Algorithm ----
------------------------------------------------
- easy to describe impossible to implement
- at moment of page fault, evict page that will be used farthest away
- can do in two passes, not one

---- The Not Recently Used Page Replacement Algorithm ----
----------------------------------------------------------
- two status bits, R and M
  R: set whenever page is referenced (read or written)
  M: set whenever the page was modified
- must be updated on every memory reference
- when process started up, both page bits for all pages set to 0 by os
- periodically clearly R bit to indicate not referenced
- on page fault, pages divided into categories
Class 0: not referenced, not modified
Class 1: not referenced, modified
Class 2: referenced, not modified
Class 3: referenced, modified
- remove a page at random from lowest numbered nonempty class

---- The First in First out Page Replacement Algorithm ----
-----------------------------------------------------------
- os maintains list of all pages in memory
  - most recent arrival at tail
  - least recent arrival at head
- on page fault, remove from head and add to tail

---- The Second Chance Page Replacement Algorithm ----
------------------------------------------------------
- modification to FIFO that insepct R bit of oldest page
- if 0, evicted and replaced
- if 1, clear bit and put page onto end of list
  - keep looking for page with 0 R to evict


---- The Clock Page Replacement Algorithm ----
----------------------------------------------
- keep pages in circular list in form of clock
- same algo as second change occurs
- page pointed to by hand inspected
- if R is 0, evicted and new page inserted
- if R is 1, clear R and advance hand


---- The Least Recently Used Page Algorithm ----
------------------------------------------------
- observation that page that have been used in last few instructions will be used again
- observation that pages that have not been used in a while will continue not to be used
- maintain list that must be updated on every memory reference
  - this is expensive and requires hardware support to be feasible

---- Simulating LRU in Software ----
------------------------------------
- software counter with each page 
- on interrupts, os scan all page and R bit added to counter for page
- page with lowest counter chosen for replacement
- modifications to make it work better


---- The Working Set Page Replacment Algorithm ----
---------------------------------------------------
- processes started up with no pages and brought in as needed
- demand paging: pages are loaded only on demand, not in advanced
- most processes exibit locality so only using a small number of pages
- set of pages processes currently using is its working set
- thrashing: program causing page faults every few instructions
- working set model: keep track of processes working set and prepage them in
- evict pages not in the working set (k most recent pages)
- can track age of page in time

---- The WSClock Page Replacement Algorithm ----
------------------------------------------------
- widely used in practice
- circular list of page frames
- each entry contains time of last use, R bit, M bit
- if R bit is zero and time of last use is greater than some threshold, evict
  - if page dirty, write to storage scheduled but hand advanced to look for another page

---- Summary of Page Replacement Algorithms ----
------------------------------------------------
- Optimal: Not implementable, but useful as a benchmark
- NRU (Not Recently Used): Very crude approximation of LRU
- FIFO (First-In, First-Out): Might throw out important pages
- Second chance: Big improvement over FIFO
- Clock: Realistic
- LRU (Least Recently Used): Excellent, but difficult to implement exactly
- NFU (Not Frequently Used): Fairly crude approximation to LRU
- Aging: Efficient algorithm that approximates LRU well
- Working set: Somewhat expensive to implement
- WSClock: Good efficient algorithm



-----------------------------------------------
---- 3.5: Design Issues For Paging Systems ----
-----------------------------------------------

---- Local versus Global Allocation Policies ----
-------------------------------------------------
- how should memory be allocated among competing runnable processes???
- Local algorithm: allocating every process fixed fraction of memory
- Global algorithm: allocating memory dynamically among runnable processes
- can allocate pages in proportion to process size, with some minimum
- PFF (Page fault frequency): tells when to increase or decrease a process' page allocation
  - fault rate decreases as more pages are assigned
- trade offs between local and global, comes down to amount of page faults


---- Load Control ----
----------------------
- working set of all processes exceed memory, but one process may need more and none need less
  - no way to give more memory
- OS has process called OOM (out of memory killer) that becomes active when system low on memory
  - score processes based on "badness" and kill ones with highest scores
- can also swap process to nonvolitaile storage to free mem, compress, and compact

---- Cleaning Policy ----
-------------------------
- paging daemon: sleeps most of time but periodically awakened to insepct state of memory
  - if too few page frames are free, selets pages to evict
- makes sure free frames are clean and do not need to be written when required
- two handed clock: one hand writing dirty pages and other for replacement


---- Page Size ----
-------------------
- on average, half of final page will be empty
  - extra wasted space called internal fragementation
  - n segements, size of p bytes, n * p / 2 wasted
  - reason for smaller page size
- small pages mean need many pages and larger page table
  - same time read/write to disk as large page
- small pages use up valuable space in tlb

---- Separate Instruction and Data Spaces ----
----------------------------------------------
- most computers have addr space that holds program and data
  - if addr space is not large enough, problems arise
- can seperate program text (I space) and data (D space)
  - although, each need own page table and mapping
  - used for L1 cache today

---- Shared Pages ----
----------------------
- sharing pages is straightfoward with I&D pages
  - two+ processes shared same I space and use different D space
- harder with single addr space
  - if two processes sharing, evicting one process & its pages will cause other to page fault
  - if one process terminates, have to figure out which pages can be cleaned up

---- Shared Libraries ----
--------------------------
- in modern systems, many systems have to share large libraries
- when program linked with shared library, small stud routine that binds to called function at runtime
included
  - functions paged in when needed

---- Mapped Files ----
----------------------
- process can issue syscall to map file onto portion of its virtual addr space
- alternate model to I/O
  - file accessed as big character array in memory



------------------------------------
---- 3.6: Implementation Issues ----
------------------------------------

---- Operating System Involvement with Paging ----
--------------------------------------------------
- has to decide how large program & data will be initially and create page table
- space needs to be allocated
- page table needs to be ready to go
- MMU rest and TLB flushed
- clean up page table
- various other duties

---- Page Fault Handling ----
-----------------------------
1) hardware trap to kernel, pc saved on stack and cpu information saved
2) ISR started to save regs and other volatile info, then call page fault handler
3) os try to discover which virtual page needed
4) once have virtual addr which casued fault, validate addr and look for free page frame
5) if page frame dirty, write to nonvolatile
6) when page frame clean, bring in needed page
7) update page table when page arrives
8) instruction backed up to state it had when it began, pc reset too
9) process is rescheduled
10) registers and other state information restored, return to user space

---- Instruction Backup ----
----------------------------
- must determine where first byte of instruction is 

---- Locking Pages in Memory ----
---------------------------------
- I/O must still occur
- chance that page containing I/O buffer will be chosen and removed from memory
- can lock/pin a page to make sure that it is not removed

---- Backing Store ----
-----------------------
- allocating space on nonvolatile storage uses swap parition
- new processes assigned chunks of swap parition
  - associated w/ it is the address of its nonvolatile storage area
- other algorithms for this



----------------------------
---- 3.7: Segementation ----
----------------------------
- having two or more seperate virtual address spaces could be better
- can provide many completely independent addr spaces: segements
- if segements ind, can grow/shrink easily
- program must supply two part address
  - segement number and addr in segment
- segements can be shared or have different protections

---- Implementation of Pure Segementation ----
----------------------------------------------
- pages are fixed size but segements are not
- allow programs and data to be broken up into logically ind addr spaces and aid sharing/protection

---- Segementation with Paging: MULTICS ----
--------------------------------------------
- only pages of segement that are actually needed have to be around
- treated each segemetn as virtual memory and page it
- segement table w/ one desciptor per segement
  - segement table itself was segemented and paged
- descriptor said if segement was in memory or not
- address has two parts: segement and address within segement
  - address within segement further divided into page number and word within page

---- Segementation with Paging: The Intel x86 ----
--------------------------------------------------
- fewer segements but larger ones
- many programs need large segements


-------------------------
---- Chapter WriteUp ----
-------------------------
- The simplest and earliest form of memory management is no page swapping at all. A progress is just run to completion.
You can expand on this by moving the entire process memory image onto novolatile storage and then loading in another process.
This was still not enough and hence virtual memory was designed. It provides a process with the illusion that it has nearly 
infinite contiguous memory but this is handled via pages and swapping mechanisms. A process will issue a virtual address which
will be translated by the MMU via the page table. This results in a physical address which is then used to address into memory.
All pages cannot be in memory at the same time so paging algorithms were designed to most effectivley keep pages in memory that are 
needed and replace them in the best manner. 






---- Memory Notes ----
----------------------
- Virtual memory: program has its own address space broken into pages, providing illusion of nearly infinite contiguous memory
- Address space: set of addresses a process can use to address memory
- MMU (Memory Management Unit): maps virtual addresses to physical memory addresses
- Page: fixed-size unit of virtual address space
- Page frame: corresponding physical memory to a page
- Page fault: occurs when MMU tries to access unmapped page, triggers OS to load needed page
- Page table: maps virtual pages to page frames
- TLB (Translation Lookaside Buffer): hardware cache for virtual-to-physical address mappings
- Soft miss: page not in TLB but in memory (10-20 machine instructions to handle)
- Hard miss: page not in memory (much slower than soft miss)
- Segmentation fault: program accesses invalid address
- Multilevel page table: hierarchical structure for large address spaces
- Inverted page table: single table for all physical memory frames
- Swapping: moving processes between memory and nonvolatile storage
- Page replacement algorithms:
  - Optimal (theoretical): evict page used farthest in future
  - NRU (Not Recently Used): uses R (referenced) and M (modified) bits
  - FIFO (First-In-First-Out): evict oldest page
  - Second Chance: modified FIFO, gives pages second chance if recently used
  - Clock: circular list version of Second Chance
  - LRU (Least Recently Used): evict least recently used page
  - Working Set: track and maintain current working set of pages
  - WSClock: efficient combination of working set and clock algorithms
- Demand paging: load pages only when needed
- Paging daemon: periodically selects pages to evict or write to memory
- Page fault handling process: hardware trap, save state, find virtual page, validate, find free frame, load page, update table, restore state
- Design considerations: local vs. global allocation, load control, cleaning policy, page size, separate I&D spaces, shared pages
- Memory management methods: bitmaps, free lists
- OOM (Out of Memory) killer: process that terminates other processes when system is low on memory






---- Summary ----
-----------------
- Along with paging comes various implementation issues. Some of these issues include configuration based issues such as how to decide the initial
size of the page table and other are implementation issues such as designing a mechanism to keep pages hot while doing I/O. This issues are often
hidden in the background but must be addresses for proper paging to occur

---- Memory Notes ----
----------------------
- Process of page fault handling?: 1) hardware trap to kernel, pc saved on stack and cpu information saved,  2) ISR started to save regs and other volatile info, then call page fault handler,
3) os try to discover which virtual page needed, 4) once have virtual addr which casued fault, validate addr and look for free page frame
5) if page frame dirty, write to nonvolatile, 6) when page frame clean, bring in needed page, 7) update page table when page arrives
8) instruction backed up to state it had when it began, pc reset too, 9) process is rescheduled, 10) registers and other state information restored, return to user space








---- Summary ----
-----------------
- There are many tradeoffs to consider when you are implementing paging algorithms. One choice can have results that trickle down to other
parts of the systems and have impacts of performance. 
 
---- Memory Notes ----
----------------------
- What are design issues for paging systems? 1) local vs global allocation: local -> allocate every process fixed fraction of memory. Global: allocating memory dynamically among runnable processes.
2) Load Control: working set of all processes may exceed memory and a process may need more. OOM (out of memory killer) rank processes and kill ones, can also compact and compress. 3) cleaning policy:
paging daemon sleeps most of time but periodically awakened to select pages to evict or write to memory. 4) page size: on average half of final pages will be empty. Dont want page size too small so we 
do a ton of swaps but also dont want too bit so we waste too much memory. 5) Seperate instruction and data spaces: can sepearate to save/share space but need own page table. 6) Shared pages: easy with
seperate I&D spaces but if not seperate have to manage how to share when one process killed. 















---- Summary ----
-----------------
- When a page fault occurs, the new page must be brought into memory. It is often the case that memory is already full of pages from other proess. When this
occurs we need to evict a page from memory to include the newly accessed page. There are various algorithms proposed for this which each have their tradeoffs. 
Ideally, we want an algorithm that will replace a page that will be used farthest in the future but this is not possible to implement in one pass. We cant reach
optimal, but there are efficient algorithms that perform this job reasonably sufficiently. 

---- Memory Notes ----
----------------------
- What is the optimal, but impossible to implement page replacement algorithm? At moment of page fault, you evict the page that will be used farthest away in the future, not implementable
- Not Recently used page replacement: R bit -> set whenever page references, M bit -> set whenever page modified. Pages divided into categories. Class 0: not references or modified.
Class 1: not referenced, modified. Class 2: references, not modified. Class 3: referenced, modified. Remove page at random from lowest numbered nonempty class, crude approx of LRU
- First in First out Page Replacement: Linked list of pages, most recent arrival at tail and oldest at head. on fault, remove from head and add to tail, might throw out important pages
- Second Chance Page Replacment: Modified FIFO, linked list like fifo but on page fault inspect R bit of oldest page (head). If 0, evict, else move it to end of list and keep looking 
- Clock Page Replacement: Keep pages in circular list. Same algorithm as second chance. If R is 0, evice and insert new page, if R is one, clear R and advance hand, realistic
- Least Recently used page replacement: page not used recently will likely not be used again soon, maintain list of references and remove least recently used page, Excellent, difficult to impl
- Working set page replacement: set of pages process using is working set, keep track of process working set and evict pages not in working set, expensive to impl
- Workign set clock page replacement: Circular list of pages, R bit = 0 and time of last use greater than some threadshold, evict, good and efficient
- demand paging: pages are loaded only on demand





---- Summary ----
-----------------
- Virtual memory is an abstraction over memory that provides each process with the illusion of having nearly unlimited access
to memory in a contiguous sequence. The virtual address spaces is broken up onto multiple pages, which each contain a section 
of the virtual address space. When a process tries to access an address in its virtual address space, that address is send to the 
memory management unit to be transformed into a physical address that can be used to access the value at that location. If the current
virtual address's page is not in memory, a page currently in memory needs to be evicted and the new one brought in. Eviction is handled 
thorugh various algorithms. Paging can be slow. You must map from a virutal address to a physical address and then have to find the associated 
page very quyickly. The translation lookaside buffer is a hardware cache that caches mappings so you can avoid a page table lookup. This helps 
speed up paging. As for the structure of page tables, the two main approaches are multilevel page tables and inverted pages tables

---- Memory Notes ----
- virtual memory: program has its own addr space that is broken into pages
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual addresses: program generated addresses that form the virtual address space
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- what happens if mmu tries to access unmapped page? page fault, trap in os, write unused page to disk and bring
in needed page
- process of an address fetch w/ virtual memory: 1) program issue adress to access byte in page, 2) mmu checks page table to see if page is mapped to physical frame, 
if mapped, retrieve data, 3) if not mapped, issue page fault bc it needs to load the page into memory 4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped 6) restart the instruction
- virtual add split into? virtual page number and offset
- what is virtual page number used for? index into page table, if page found, page frame attatched to offset to form physical addr
- purpose of page table? map virtual pages onto page frames
- what is the purpose of virtual memory? create an abstraction over the address space, which is an abstraction of physical memory
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table, inside the mmu
- process when address presented to mmu for tranlsation: 1) hardward checks to see if virtual page number in TBL (done in parallel via hardware), 
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory, 3) if no match found, 
mmu does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 
- multilevel page table: hierarchical page table structure, poritions of virtual addr used to index into various levels of page table, used for handling
large address spaces while minimizing the memory required for the page table itself
- inverted page table: optimize amount of memory used by page table itself, single table for all physical memory frames, each entry corresponds to physical frame and
stores info about which virtual frame it holds









---- Summary ----
-----------------
- The most basic memory abstration is the concept of an address space which is just
a set of addresses that a process can use to address its memory. Each process has its own address
space, so we need to have some way to fit all of the address spaces in memory when there are multiple processes
running. There are various algorithms to achieve this but none of them presented here are optimal, but the
concept of swapping is important in the final solution

---- Memory Notes ----
----------------------
- address space: set of addresses that a process can use to address memory
- addr space swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- mem mgmt w/ bitmap: memory divded into allocation units in size from a few words to several kilobytes, each allocation unit 
has bit, 0 for free and 1 for occupied, searching bit map is slow
- mem mgmt w/ linked list: linked list of allocated and free memory segements, segement either process or 
empty hole between two processes, several algorithms for allocation. first fit, next fit, best fit, quick fit, ...








---- Summary ----
-----------------
- Early computers didnt have any abstraction over memory and just ran
all processes in the same physical memory space.

---- Memory Notes ----
----------------------
- None



---- Memory Notes ----
- Memory management: crucial part of OS, manages memory hierarchy and creates memory abstraction for processes
- Simplest memory management: no abstraction, programs see physical memory directly
- Multiprogramming: multiple programs running simultaneously on one CPU, switches quickly between processes
- Address space: set of addresses that a process can use to address memory
- Virtual memory: program has its own address space broken into pages, providing illusion of nearly infinite contiguous memory
- MMU (Memory Management Unit): hardware that maps virtual addresses to physical memory addresses
- Page: fixed-size unit of virtual address space
- Page frame: corresponding physical memory to a page
- Page fault: occurs when MMU tries to access unmapped page, triggers OS to load needed page
- Page table: maps virtual pages to page frames
- TLB (Translation Lookaside Buffer): hardware cache for virtual-to-physical address mappings, crucial for performance
- Soft miss: page not in TLB but in memory (10-20 machine instructions to handle)
- Hard miss: page not in memory (much slower than soft miss)
- Segmentation fault: program accesses invalid address
- Multilevel page table: hierarchical structure for large address spaces
- Inverted page table: single table for all physical memory frames
- Swapping: moving processes between memory and disk when memory is full
- Page replacement algorithms:
  - Optimal: evict page used farthest in future (theoretical, not implementable)
  - NRU (Not Recently Used): uses R (referenced) and M (modified) bits
  - FIFO (First-In-First-Out): evict oldest page
  - Second Chance: modified FIFO, gives pages second chance if recently used
  - Clock: circular list version of Second Chance
  - LRU (Least Recently Used): evict least recently used page
  - NFU (Not Frequently Used): approximation of LRU
  - Aging: better approximation of LRU
  - Working Set: track and maintain current working set of pages
  - WSClock: efficient combination of working set and clock algorithms
- Demand paging: load pages only when needed
- Thrashing: excessive paging, occurs when working set exceeds available memory
- Working set: set of pages a process is currently using
- PFF (Page Fault Frequency): algorithm to determine when to increase/decrease a process' page allocation
- Local vs. global page replacement: local allocates fixed memory to each process, global dynamically allocates
- Cleaning policy: use of a paging daemon to maintain a supply of free page frames
- Page size considerations: trade-off between fragmentation and overhead
- Separate I and D spaces: separate address spaces for instructions and data
- Shared pages: allow multiple processes to share common code or data
- Copy-on-write: optimization technique for efficient process creation and memory usage
- Shared libraries: reduce memory usage and disk space by allowing multiple processes to share common code
- Memory-mapped files: allows file I/O to be treated as memory access
- Segmentation: allows logical divisions of memory (e.g., code, data, stack), supports sharing and protection
- External fragmentation: waste of memory due to variable-sized segments
- Paged segmentation: combines advantages of paging and segmentation (e.g., MULTICS, x86 pre-x86-64)
- OOM (Out of Memory) killer: process that terminates other processes when system is low on memory
- Page table entry structure: typically includes page frame number, present/absent bit, modified bit, referenced bit, protection bits
- Instruction backup: necessary to restart instructions after page faults
- Locking pages: preventing certain pages from being swapped out, important for I/O operations
- Backing store: area on disk used for storing pages not currently in memory
- Separation of policy and mechanism: important principle in memory management implementation
- Prepaging: loading pages into memory before they are referenced, to reduce initial page faults
- Memory compaction: moving occupied memory blocks together to reduce external fragmentation
- Internal fragmentation: wasted space within allocated memory blocks due to fixed page sizes
- Base and limit registers: simple method for implementing virtual memory, used in early systems
- Address translation: process of converting virtual addresses to physical addresses
- Page table walks: process of traversing page table hierarchy to resolve virtual addresses
- Transparent huge pages: OS feature to automatically use larger page sizes when beneficial
- Deduplication: technique to reduce memory usage by sharing identical memory pages between processes
- NUMA (Non-Uniform Memory Access): memory design where memory access time depends on the memory location relative to the processor
- Memory pressure: condition when the system is running low on free memory
- Kernel page table isolation (KPTI): security feature to protect against certain CPU vulnerabilities like Meltdown